{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "# Modelling\n",
    "from sklearn.metrics import accuracy_score,precision_score,f1_score,roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining input and Output features of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label']\n",
    "X = df.drop('label',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copying the value of X in messages for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = X.copy()\n",
    "messages.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "lm  = WordNetLemmatizer()\n",
    "corpus =[]\n",
    "for i in range(0,len(messages)):\n",
    "    review = re.sub('[^A-za-z]','',str(messages['title'][i]))\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lm.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ''.join(review)\n",
    "    corpus.append(review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ripeshghimire/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Reshape the data to be a single column with each word as a row\n",
    "corpus_array = [[word] for word in corpus]\n",
    "\n",
    "# Fit and transform the data\n",
    "onehot_encoded = onehot_encoder.fit_transform(corpus_array)\n",
    "onehot = onehot_encoded.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(onehot,y,train_size=0.3,random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(true, predicted):\n",
    "    f1 = f1_score(true, predicted)\n",
    "    roc_score = roc_auc_score(true, predicted)\n",
    "    accuracy = accuracy_score(true, predicted)\n",
    "    return f1, roc_score, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier\n",
      "Model performance for training set\n",
      "F1 Score (Training): 0.9243\n",
      "ROC Score (Training): 0.9171\n",
      "Accuracy Score (Training): 0.9176\n",
      "----------------------------\n",
      "Model Performance for test data\n",
      "F1 Score (Test): 0.6662\n",
      "ROC Score (Test): 0.5000\n",
      "Accuracy Score (Test): 0.4995\n",
      "===================================\n",
      "\n",
      "\n",
      "DecisionTree\n",
      "Model performance for training set\n",
      "F1 Score (Training): 1.0000\n",
      "ROC Score (Training): 1.0000\n",
      "Accuracy Score (Training): 1.0000\n",
      "----------------------------\n",
      "Model Performance for test data\n",
      "F1 Score (Test): 0.2035\n",
      "ROC Score (Test): 0.5566\n",
      "Accuracy Score (Test): 0.5571\n",
      "===================================\n",
      "\n",
      "\n",
      "RandomForestClassifier\n",
      "Model performance for training set\n",
      "F1 Score (Training): 1.0000\n",
      "ROC Score (Training): 1.0000\n",
      "Accuracy Score (Training): 1.0000\n",
      "----------------------------\n",
      "Model Performance for test data\n",
      "F1 Score (Test): 0.2035\n",
      "ROC Score (Test): 0.5566\n",
      "Accuracy Score (Test): 0.5571\n",
      "===================================\n",
      "\n",
      "\n",
      "AdaBoost\n",
      "Model performance for training set\n",
      "F1 Score (Training): 0.1834\n",
      "ROC Score (Training): 0.5505\n",
      "Accuracy Score (Training): 0.5476\n",
      "----------------------------\n",
      "Model Performance for test data\n",
      "F1 Score (Test): 0.1423\n",
      "ROC Score (Test): 0.5383\n",
      "Accuracy Score (Test): 0.5387\n",
      "===================================\n",
      "\n",
      "\n",
      "SVC\n",
      "Model performance for training set\n",
      "F1 Score (Training): 1.0000\n",
      "ROC Score (Training): 1.0000\n",
      "Accuracy Score (Training): 1.0000\n",
      "----------------------------\n",
      "Model Performance for test data\n",
      "F1 Score (Test): 0.2035\n",
      "ROC Score (Test): 0.5566\n",
      "Accuracy Score (Test): 0.5571\n",
      "===================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "models = {\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"SVC\": SVC(),\n",
    "}\n",
    "\n",
    "model_list = []\n",
    "accuracy = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)  # Train model\n",
    "\n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate train and test dataset\n",
    "    model_train_f1, model_train_roc, model_train_accuracy = evaluate_model(y_train, y_train_pred)\n",
    "    model_test_f1, model_test_roc, model_test_accuracy = evaluate_model(y_test, y_test_pred)\n",
    "\n",
    "    print(model_name)\n",
    "    \n",
    "    print(\"Model performance for training set\")\n",
    "    print(\"F1 Score (Training): {:.4f}\".format(model_train_f1))\n",
    "    print(\"ROC Score (Training): {:.4f}\".format(model_train_roc))\n",
    "    print(\"Accuracy Score (Training): {:.4f}\".format(model_train_accuracy))\n",
    "    print(\"----------------------------\")\n",
    "    \n",
    "    print(\"Model Performance for test data\")\n",
    "    print(\"F1 Score (Test): {:.4f}\".format(model_test_f1))\n",
    "    print(\"ROC Score (Test): {:.4f}\".format(model_test_roc))\n",
    "    print(\"Accuracy Score (Test): {:.4f}\".format(model_test_accuracy))\n",
    "    \n",
    "    model_list.append(model_name)\n",
    "    accuracy.append(model_test_accuracy)\n",
    "\n",
    "    print('=' * 35)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
